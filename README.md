# ğŸ›°ï¸ AI-Sentinel

> **Evaluate Â· Monitor Â· Guard your AI systems**  
> Self-hosted evaluation & monitoring service for LLM outputs.  
> Built with **FastAPI + Postgres/pgvector + Tailwind dashboard**.

---

## ğŸš¨ Why AI-Sentinel?

LLMs donâ€™t fail loudly â€” they **hallucinate, regress, or leak data** silently.  
**AI-Sentinel** is your watchdog: it continuously **evaluates, monitors, and guards** your AI outputs.

- **Evaluate** â†’ Run regression tests against your own datasets & model endpoints  
- **Monitor** â†’ Score live production outputs in real-time  
- **Guard** â†’ Detect JSON errors, PII leaks, missing citations, hallucinations, and unsafe outputs  
- **Track** â†’ Pin baselines and diff model versions for regressions  

---

## ğŸ” How AI-Sentinel compares

| Tool         | Hosting      | Checks out-of-box              | UI/Dashboard | Cost model          | Notes                                    |
|--------------|-------------|--------------------------------|--------------|--------------------|------------------------------------------|
| **AI-Sentinel** | Self-hosted | JSON validity, regex, PII, length, similarity, toxicity stub | âœ… Tailwind dashboard | Free (your infra only) | API-first, lightweight, works with any model |
| LangSmith    | SaaS        | Many, incl. custom evals       | âœ… Polished   | Paid per trace      | Deep LangChain integration, vendor lock-in |
| TruLens      | Lib + SaaS  | Attribution, scoring           | Limited      | Free + SaaS tiers   | Python-first, less monitoring focus        |
| Ragas        | Lib         | RAG-focused evals              | âŒ No UI     | Free (OSS)          | Python-only, no monitoring                 |
| Arize Phoenix| OSS         | Embedding metrics, drift       | âœ… Advanced   | Free OSS + SaaS     | Observability heavy, steeper learning curve|

ğŸ‘‰ Choose **AI-Sentinel** if you want a **lean, open, self-hosted guardrail** for AI outputs â€” not another heavy SaaS with surprise trace bills.

---

## âœ¨ Current Features
- Project registration (dataset + inference endpoints)
- Run orchestration (pull dataset â†’ call inference â†’ run checks)
- Built-in checks:
  - âœ… JSON validity (with optional schema)
  - âœ… Regex policy (required / forbidden patterns)
  - âœ… PII detection (emails, phones, credit-cards, addresses)
  - âœ… Length bounds
  - âœ… Semantic similarity (optional, via OpenAI embeddings)
  - âœ… Toxicity (simple wordlist stub)
- Baseline management & regression diffs
- Monitoring endpoint (`/v1/monitor/events`)
- Minimal Tailwind dashboard for runs & stats

---

## ğŸ›£ï¸ Roadmap

### Near-term
- [ ] Dashboard filters, charts, and historical pass-rate trends  
- [ ] Prometheus metrics & healthcheck endpoints  
- [ ] Richer toxicity & safety classifiers  
- [ ] One-click demo mode (spawn dataset + inference inside service)  

### Mid-term
- [ ] CI/CD integration (GitHub Action for evals on PRs)  
- [ ] Slack/Teams alerts on regressions or prod failures  
- [ ] Pluggable check registry (drop in your own checks)  

### Long-term
- [ ] Multi-tenant SaaS mode (orgs, users, billing)  
- [ ] RAG-specific evals (faithfulness, context grounding)  
- [ ] Full compliance/audit reporting  

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/yourname/ai-sentinel
cd ai-sentinel
docker compose up --build


  â€¢ Visit http://localhost:8000 â†’ Dashboard
  â€¢ Visit http://localhost:8000/docs â†’ Swagger API



ğŸ§ª Demo Mode

Point to the built-in demo endpoints:
  â€¢ Dataset: http://api:8000/demo/tests
  â€¢ Inference: http://api:8000/demo/infer



curl -X POST http://localhost:8000/v1/projects \
  -H "Content-Type: application/json" \
  -d '{"name":"demo","dataset_url":"http://api:8000/demo/tests","inference_url":"http://api:8000/demo/infer"}'

curl -X POST http://localhost:8000/v1/runs \
  -H "Content-Type: application/json" \
  -d '{"project_id":"<project-id>"}'


Then refresh the dashboard (/) or check GET /v1/runs/<id>/report.



ğŸ“œ License

MIT
---

âš¡ This positions **AI-Sentinel** as a *self-hosted, lean alternative to LangSmith & friends* with a clear **Evaluate Â· Monitor Â· Guard** tagline.  

Do you also want me to bake this new README **into your code bundle** and update the dashboard branding (logo/title) to say **AI-Sentinel** instead of â€œLLM Eval Serviceâ€?

